%%%  TO FORMAT A PREPRINT To submit to a PCI.  If accepted then use this main.tex with the package of accepted article and search for [IF ACCEPTED] comments  %%%

\documentclass[a4paper]{article}

%%%   SET THE TITLE   %%%
\newcommand{\preprinttitle}{Analysis of the Abundance of Radiocarbon Samples without the Sum of Probability Distributions}
%%%%%%%%%%%%%%%%%%%%%%%%%


%%%  SET THE LIST OF AUTHORS WITH CORRESPONDING AFFILIATIONS  use \& before last author %%%
\newcommand{\listauthors}{\raggedright 
Miguel de Navascu√©s\textsuperscript{1,2,\includegraphics[height=0.25cm]{ORCID.pdf}},
Concetta Burgarella\textsuperscript{2,3,\includegraphics[height=0.25cm]{ORCID.pdf}}
\& Mattias Jakobsson\textsuperscript{2,\includegraphics[height=0.25cm]{ORCID.pdf}}
}
%%%%%%%%%%%%%%%%%%%%



%%%  SET THE LIST OF AFFILIATIONS  %%%
\newcommand{\listinstitutions}{
\textsuperscript{1} CBGP, INRAE, CIRAD, IRD, Institute Agro, University of Montpellier, Montpellier, France
\\
\textsuperscript{2} Human Evolution Program, Department of Organismal Biology, Uppsala University, Uppsala, Sweden
\\
\textsuperscript{3} AGAP Institut, University of Montpellier, CIRAD, INRAE, Institut Agro, Montpellier, France
\\
\includegraphics[height=0.3cm]{ORCID.pdf} MdN, \href{http://orcid.org/0000-0001-8342-6047}{0000-0001-8342-6047}; CB, \href{http://orcid.org/0000-0003-4522-4297}{0000-0003-4522-4297}; MJ, \href{http://orcid.org/0000-0001-7840-7853}{0000-0001-7840-7853}
}
%%%%%%%%%%%%%%%%%%%%%%



%%%  SET THE 'CORRESPONDENCE TO'  %%%
\newcommand{\email}{miguel.navascues@inrae.fr}
%%%%%%%%%%%%%%%%%%%%%%

%%%  SET THE ABSTRACT %%%
\newcommand{\preprintabstract}{xxx
%The study of the abundance of radiocarbon dates has emerged as a promising source of information for archaeology. However, until recently, the analysis of these data lacked the use of statistical tools that allow for model choice, model fitting and quantification of uncertainty. In this work we propose a generative model for the dated archaeological record and the use of approximate Bayesian computation to perform model choice and parameter inference under that model. The application of this approach provides a quantitative evaluation of models and parameters, which allows for an interpretation of the data that goes beyond verbal description of patterns. As an example, the method is applied to a previously published dataset or radiocarbon dates from Britain and Ireland. The results offer an estimation of the abundance of radiocarbon dates through time that includes credible intervals to quantify the uncertainty and identify the periods in which the rate of change is significant. Our results provide no evidence of a relationship with climate (temperature). The application of the methods to the subset of samples constituted by wheat and barley shows that the dynamics of the abundance of samples for those two cereals is not independent and a significant shift in their relative abundance. This work provides a formal model for the description of the dated archaeological record, with parameters that have an intuitive meaning unlinked to the usual demographic interpretation. We also proposed the use of approximate Bayesian computation as a flexible framework to test specific hypothesis and identify period of interest.
}
%%%%%%%%%%%%%%%%%%%%%%



%%%  SET THE KEYWORDS  %%%
\newcommand{\preprintkeywords}{radiocarbon dating; demography; likelihood-free inference; simulation}
%%%%%%%%%%%%%%%%%%%%%%




%%% [IF ACCEPTED] UNCOMMENT TO CHOOSE A PCI %%%
%\newcommand{\PCI}{Peer Community In Zoology}
%\newcommand{\PCI}{Peer Community In Ecology}
%\newcommand{\PCI}{Peer Community In Evolutionary Biology}
%\newcommand{\PCI}{Peer Community In Paleontology}
%\newcommand{\PCI}{Peer Community In Archaeology}
%\newcommand{\PCI}{Peer Community In Animal Science}
%\newcommand{\PCI}{Peer Community In Genomics}
%\newcommand{\PCI}{Peer Community In Neuroscience}
%\newcommand{\PCI}{Peer Community In Mathematical and Computational Biology}
%\newcommand{\PCI}{Peer Community In Forest and Wood Sciences}
%\newcommand{\PCI}{Peer Community In Ecotoxicology and Environmental Chemistry}
%\newcommand{\PCI}{Peer Community In Infections}
%\newcommand{\PCI}{Peer Community In Registered Reports}
%\newcommand{\PCI}{Peer Community In Network Science}
%\newcommand{\PCI}{Peer Community In Microbiology}
%%%%%%%%%%%%%%%%%%%%%

%%%  [IF ACCEPTED] UNCOMMENT AND CHOOSE "yes" or "no" if your article contains data for which you provided the link to raw data, and {statistical script or computer code} %%%
%\newcommand{\opendata}{yes}
%\newcommand{\opencode}{yes}
%%%%%%%%%%%%%%%


%%%  [IF ACCEPTED] UNCOMMENT AND SET THE version of the preprint. Replace X by a number   %%%
%\newcommand{\version}{X}
%%%%%%%%%%%%%%%%%%


%%%  [IF ACCEPTED] UNCOMMENT AND SET THE DATE OF LATEST UPLOAD on the preprint server    %%%
%\newcommand{\datepub}{ddth Month yyyy}
%%%%%%%%%%%%%%


%%%  [IF ACCEPTED] UNCOMMENT AND SET THE RECOMMENDER(s) NAME(s)  %%%
%\newcommand{\recommender}{FirstName FamilyName}
%%%%%%%%%%%%%%%%%

%%%  [IF ACCEPTED] UNCOMMENT AND SET THE DOI of the RECOMMENDATION  %%%
%\newcommand{\DOIrecommendation}{xxx/xxx}
%%% for example 10.24072/pci.mcb.100003 %%%


%%%  [IF ACCEPTED] UNCOMMENT AND SET THE REVIEWERS' NAMES IF KNOWN and/or X anonymous reviewers  %%%
%\newcommand{\reviewers}{FirstName FamilyName and two anonymous reviewer}
%%%%%%%%%%%%%%%


%%%   [IF ACCEPTED] UNCOMMENT AND SET THE 'CITE AS' OF YOUR PREPRINT. paste the line we sent you by Email (XXX the "cite as") in place of the xxx  %%%
%\newcommand{\citeas}{xxx}
%%%%%%%%%%%%%%%%%

\input{preambule.tex}

\usepackage{gensymb}
\usepackage{amsmath}
%\usepackage{nath}
\usepackage{tabularx}

% for cell colors
\newcommand{\customcell}[1]{\cellcolor{pciblue!\the\numexpr#1/300\relax!white!100}{$#1$}}


\begin{document}
\beginingpreprint

%%%  [IF ACCEPTED] COMMENT linenumbers  %%%
\linenumbers
%%%%%%%%%%%%%%%%%%

%%%  TEXT OF THE PREPRINT  %%%
% use \emph{} for italics and \parencite{} or \textcite to cite a reference and \ref{XX} to cite the \label{XX}
% use \\ and a blank line to mark the end of paragraph and starting of a new one

\section*{\centering Introduction}


The development of radiocarbon dating \parencite{Libby1949} has revolutionized the study of the past, finding applications in archaeology, geology, paleobiology, and paleoclimatology \parencite{Taylor1995,BronkRamsey2008,Carleton2021}. As this technique became a standard in research, the accumulation of dated samples has led to the investigation of sample abundance over time, addressing various questions related to environmental processes \parencite[\emph{e.g.} changes in sea level, forest fire frequency, or fluvial activity;][]{Geyh1980, Pierce2004, Thorndycraft2006}, as well as studying population size changes in humans and other species \parencite[\emph{e.g.}][]{Rick1987,Broughton2018}. There is a growing interest in the analysis of radiocarbon sample abundance, notably fueled by the recent availability of extensive \textsuperscript{14}C databases \parencite[e.g.][]{Bird2022} and the development of new statistical methods \parencite[reviewed in][]{Crema2022}.
\\


Until very recently, the analysis of radiocarbon data abundance relied predominantly on the "sum of probability distributions" (SPD). The SPD is derived by aggregating the posterior distributions for the calibrated age of each sample in the dataset. However, the interpretation of the SPD encounters a main challenge because it lacks a precise definition of its underlying meaning. Despite speculation by some authors on the significance of such sums of probabilities, a formal mathematical definition is notably absent \parencite[\emph{e.g.}][; also see the supplementary text~\ref{supplementary_text}]{Carleton2021,Crema2022}. Despite this lack of a clear interpretation, the SPD is considered informative regarding changes in radiocarbon sample abundance over time. Nevertheless, the absence of a formal model hinders the full use of this intuition, as there is no established measure of the significance and uncertainty associated with variations in the SPD.
\\


In recent years, significant progress has been made with the introduction of model-based methods, as extensively reviewed by \textcite{Crema2022}. This advancement has opened up new avenues for analyzing the abundance of radiocarbon dates, enabling the testing of models, making model comparisons, and estimating model parameters. However, these innovative approaches still heavily rely on the SPD or a probabilistic interpretation thereof. In these models, radiocarbon dates are conceptualized as independent samples drawn from a probability distribution, with the expectation that the distribution's shape mirrors that of the normalized SPD. Such models serve either to generate pseudo-observed data or as the foundation for inference \parencite{Porcic2020, Carleton2021b, Crema2021b, Timpson2020}. However, interpreting the SPD as a probability distribution overlooks the inherent nature of radiocarbon data, which is essentially count data. The number of samples (whether total or within a specific period) is an outcome of the process, not a fixed parameter determined by the experiment or researcher. Consequently, models that assume a fixed number of samples fail to fully account for the inherent uncertainty associated with the sampling process.
\\
 
% It is worth noting that \textcite{Carleton2021b} proposes an inference model for count `data'; however, it is not directly applied to the observed counts but to simulated counts based on a parametric bootstrap that uses the SPD as the model mentioned above. % REPESCAR PARA LA DISCUSION: Also, the parameters of that inference model, based on a negative binomial, do not have a natural interpretation to the studied process.

%In fact, an interpretation of the SPD as the number of samples from each year is more natural and useful. Consider the meaning of the SPD value at a single year. Each sample from the dated archaeological record has a probability for being `sampled' that year (a number of them can have a probability close to zero). Therefore, each of the samples that really are from that year can be considered `successes', and the samples from other years as `failures', from independent binomial trials. The sum of those successes follows a Poisson-binomial distribution, whose expected value is the sum of the probabilities of each trial (\emph{i.e.}\ the value of the SPD for that year). This reasoning provides the intuition that the SPD somehow quantifies the expected number of samples for each; and that the analysis of radiocarbon abundance data should focus in modelling the number of samples at each year. However, SPD values from different years are not independent, and, therefore, the Poisson-binomial model is not applicable to the whole SPD.
%\\

In this study, we advocate for a departure from the use of the SPD or methods derived from it in statistical inference for the analysis of radiocarbon data abundance. Instead, we propose a novel model that characterizes the expected number of samples per year, accounting for calibration uncertainty. The parameters of this model offer a natural interpretation in the context of the studied process. Inference within this model is executed within the approximate Bayesian computation framework, and its application to pseudo-observed data allows for an exploration of differences compared to approaches inspired by the SPD.
\\

To illustrate the application of our proposed model, we reanalyze a published dataset of archaeological radiocarbon dates from Britain and Ireland \parencite{Bevan2017a}. This case study serves as an example of the practical application of our approach, shedding light on its potential advantages over the SPD and other methodologies. Our results are in congruence with those by \textcite{Bevan2017a}, but providing formal statistical support to the conclusions. 
\\

%Most applications of these approach (including the example data) are in the context of archaeological demographic inference and the rest of the article will focus on that application of the model.
%is only valid if we consider a single year. A single sample cannot have different ages, so several years cannot be modeled with independent Poisson binomial distributions. 

%In addition, computational approaches based on summary statistics of the data (\emph{i.e.} approximate Bayesian computation) use the SPD values at each year as such summaries \parencite[e.g.][]{Porcic2020}.


%There are, however, important limitations to this approach. For instance, \textcite{Williams2012} list five major challenges: (i) presence of biases associated to sampling within an archaeological site, (ii) limited number of samples available, (iii) influence of the calibration curve shape, that can create artificial concentration or overdispersion of dates, (iv) bias due to taphonomic loss and (v) the need to contrast results with independent data. 

%Demographic inference from radiocarbon dates would benefit from a model-based statistical framework that could offer solutions to some of these problems. A probabilistic model of the data can allow incorporating processes that influence the data such as variation of atmospheric \textsuperscript{14}C through time (i.e. calibration) or the taphonomic loss. It would also allow to quantify uncertainty of estimates for small sample sizes and offer model-test tools that could be used to test specific hypothesis based on independent data.  

%Unfortunately, the application of model-based inference from radiocarbon data is hindered by the fact that \emph{dates are not data}. Radiocarbon dating of an item is typically reported a single age value and an uncertainty quantification of that value (standard deviation). However, the true underlying data is the measure of the proportion of \textsuperscript{14}C/\textsuperscript{12}C in the sample, which is then transformed into a ``radiocarbon date'' following some machine-specific calibrations and assuming atmospheric \textsuperscript{14}C constant through time and geography. 


%ABC offer a natural way of treating them as transformation of the raw data into summaries is an integral part of the ABC process.


\section*{\centering Material and methods}

%The SPD has a central role in most of the current approaches for the analysis of the abundance of archaeological samples through time. Therefore, it is important understand the relationship between the SPD and the models discussed. A first point to clarify is the discrete nature of SPD and the probability distributions that compound it. They are sometimes described as curves of (sum of) probability density (\emph{i.e.}\ continuous distributions), but they are actually discrete distributions with one probability (or sum of probabilities) value for each year. This is a consequence of the granularity of the calibration curves, whose level of resolution is one year (at least for the periods of interest in this work).\\

\subsection*{Models for the abundance of radiocarbon samples}

\subsubsection*{Model of counts}

In the newly proposed model, the radiocarbon-dated samples are represented as a vector $\bm{R}$ (refer to Table~\ref{tab:notation} for notation in the article). Each element $R_t$ of vector $\bm{R}$ denotes the number of samples at each year $t$ within a specified date range $\left[t_{\min},t_{\max}\right]$. The abundance of radiocarbon samples is conceptualized as a Poisson distribution ($R_t \sim \mathrm{Poisson}(\lambda_t)$). This model offers a straightforward formulation with interpretable parameters: assuming that at each year $t$, there were a potential number of items $n_t$ that could contribute to the dataset with a probability $p_t$, the rate parameter $\lambda_t=n_tp_t$ represents the expected number of samples at year $t$. In the context of archaeological data, for example, $n_t$ would encompass all organic objects associated with or connected to human activity, qualifying as anthropogenic samples. The probability $p_t$ encapsulates a multifaceted process, including sample deposition, preservation over time, discovery or sampling, and decision to conduct radiocarbon dating. Consequently, vector $\bm{\lambda}$ encapsulates this intricate data-generating process, with its values representing the expected values for vector $\bm{R}$ (see figure~\ref{fig:model}a and b for a visual representation).
\\
%This model represents a simplification of the reality: what constitutes a single item in $n_t$ can be ambiguous and the probability of an object to become part of the dated archaeological record will strongly depend on its nature (\emph{i.e.}\ durability of the material or archaeological interest of the object). However, 

However, $\bm{R}$ is not directly observable because the true age of each radiocarbon sample is unknown. Radiocarbon dating provides the Conventional Radiocarbon Ages (CRA), also referred to as uncalibrated dates. The CRA values would correspond to the true dates if the environment \textsuperscript{14}C/\textsuperscript{12}C proportions were constant through time and geography and equal to that of the atmosphere in 1950; if the true \textsuperscript{14}C half-life was \citeauthor{Libby1949}'s \parencite*{Libby1949} estimate of 5730 years \parencite{BronkRamsey2008}, and if the \textsuperscript{14}C proportions were measured without error. In reality, these assumptions do not hold. Thus, radiocarbon data has the form of vector $\bm{R'}$, which contains the number of samples, $R'_u$, dated at uncalibrated date $u$ within a range $\left[u_{\mathrm{min}},u_{\mathrm{max}}\right]$ (figure~\ref{fig:model}c). The relationship between $\bm{R}$ and $\bm{R'}$ is given by the calibration curve, as in well established Bayesian analysis of radiocarbon dates \parencite{BronkRamsey2008}. Assuming that radiocarbon dating uncertainly can be modelled with a normal distribution, the radiocarbon age $u$ of a sample of age $t$ is modelled as $u \sim \mathcal{N}\left(u_{\mathrm{c},t},\sqrt{e_{\mathrm{c},t}^2+e_{\mathrm{CRA}}^2}\right)$, where $u_{\mathrm{c},t}$ and $e_{\mathrm{c},t}$ are the values of the calibration curve for time $t$ and $e_{\mathrm{CRA}}$ is the measurement error in the CRA for that sample. Using this normal distribution it is possible to model the observed number of uncalibrated dates ($\bm{R'}$) from the expected number of samples contributed by each `calibrated' year ($\bm{\lambda}$).
\\


% Calibration curves can be used to transform an uncalibrated date into a probability distribution for calibrated dates. Aggregating all these probability distributions obtained from $\bm{R'}$ we can obtain the so-called Sum of Probability Distributions (SPD). The SPD is interpreted as being informative about the abundances in $\bm{R}$, but there is no formally established correspondence between SPD and $\bm{R}$: $\bm{R}$ cannot be obtained from $\bm{R'}$. In contrast, it is straightforward to simulate $\bm{R'}$ from $\bm{R}$ using the calibration curves \parencite[][; see below]{Shennan2013}.

\begin{figure}[tbhp]
\center\includegraphics[width=10cm]{../../results/method_evaluation/plot_lambda_model.pdf} \includegraphics[width=10cm]{../../results/method_evaluation/plot_R.pdf} \includegraphics[width=10cm]{../../results/method_evaluation/plot_Rprime.pdf}
\caption[Figure with three panels. Panel "a" shows the values of lambda for every year between 2450 and 2400 BP, following an increasing exponential curve. Panel "b" shows an histogram of the number of samples at each year (simulated from the lambda values in panel "a"); the histogram is somehow ragged but follows a trend of increasing values towards more recent ages. Panel "c" show an histogram of the number of samples at each uncalibrated year (simulated from ages in panel "b"): the histogram is more noisy, the pattern of increase towards more recent ages is difficult to appreciate.]{\textbf{Model for the abundance of radiocarbon samples (example).} (a) A mathematical law determines the relationship between the expected number of samples per year ($\lambda$, the rate parameter of a Poisson distribution) and time ($t$): in this example an exponential law with initial value $\lambda_0=1$ at time $t_0=2450\mathrm{YBP}$ (\emph{i.e.}\ $t=t_0-t_{\mathrm{YBP}}$) and growth rate $r=0.04$. (b) Number of samples per year (true age) in the data set ($\bm{R}$, not observable) of one random realization of model in (a); that is, random draws from Poisson distributions with parameters in $\bm{\lambda}$. (c) Number of samples per year (conventional radiocarbon age, CRA) in the data set ($\bm{R'}$); that is, random draws from Normal distributions with parameters determined by the calibration curve and ages in (b).}
\label{fig:model}
\end{figure}



\subsubsection*{Changes through time of the abundance of radiocarbon samples}

The model for the abundance of radiocarbon samples as described above is determined by the set of parameters $\lambda_t$ in $\bm{\lambda}$. In most cases, periods of hundreds if not thousands of years will be analysed, which makes models with large number of parameters (one $\lambda$ per year). This is impractical because large amounts of data would be necessary to fit that many parameters and there would be a very likely risk of over-fitting the model. Instead, additional models can be used to determine the change of $\lambda$ through time, assuming that consecutive years will have similar $\lambda$ values. In this work, three models are explored. The first two are the exponential model ($\lambda_t = \lambda_0 e^{-rt}$, as in figure~\ref{fig:model}a) and the logistic model ($\lambda_t = \frac{k\lambda_0}{\lambda_0+(k-\lambda_0)e^{-rt}}$). These are simple models often associated to demographic processes and used in the context of the analysis of abundance of radiocarbon samples \parencite[\emph{e.g.}][]{Bevan2017a}. For a demographic interpretation of the changes in abundance of radiocarbon samples, the parameters of these models represent the initial population size ($N_0 = C \lambda_0$), the carrying capacity ($K = C k$) and the growth rate ($r$), with $C$ being an unknown constant of proportionality.
\\

%Noting the limitations of using simple models, \textcite{Brown2017} proposed a dynamic model in which an exponential growth rate changes through time depending on the population size and a covariate based on climate (temperature deviation from the optimum). As our second model, we consider a simplification of that dynamic model:
%$\lambda_t = \left\{
%    \begin{array}{lr}
%        \lambda_0, & \text{if } t=0\\
%        \lambda_{t-1} e^{\beta_0 - \beta_1\lambda_{t-1}^2 - \beta_2D^2}, & \text{if %} t\geq 1
%    \end{array}\right.$, where $\beta_0$ represents the intrinsic growth rate, $\beta_1<0$ limits growth with increasing values of $\lambda$, and $\beta_2<0$ limits growth with increasing departures from the environmental optimum ($D=0$). At the environmental optimum, $K=C\sqrt{\frac{\beta_0}{\beta_1}}$ represents the maximum carrying capacity of the population, and at low population size ($\lambda_{t-1} \rightarrow 0$), $\sqrt{\frac{\beta_0}{\beta_2}}$ represents the maximum environmental deviation from the optimum that the population can tolerate without shrinking in size. In this model, we use as a climatic covariate the deviation, $D$, of the temperature to an optimum value, $d$; \emph{i.e.}\ $D=|T-d|$ \parencite[][considered $d$ to be the average temperature of the period]{Brown2017}. Temperature values were taken from the values inferred from isotopes from an ice core in central Greenland \parencite{Alley2000,Alley2004}. The temperature values were intrapolated for each year of the period as in \textcite{Brown2017}.
%\\

However, assuming that a single mathematical function governs the changes in $\lambda_t$ over large periods of time might not be appropriate. Piecewise models can be used to set a different relationship between $\lambda$ and $t$ at different periods. The whole range of time considered $\left[t_{\min}, t_{\max}\right]$ is divided in $m$ periods defined  by $m + 1$  times $t_0, t_1, \dots, t_{m}$ (with $t_0 = t_{\mathrm{min}}$ and $t_{m} = t_{\mathrm{max}}$). Here, we consider a piecewise exponential model defined by $m + 1$ parameters $\lambda_{t_0},\lambda_{t_1},\dots ,\lambda_{t_{m}}$. Within each period $x\in\left[1,m\right]$, $\lambda$ changes exponentially with rate $r_x=\frac{\log\left(\lambda_{t_x}\right)-\log\left(\lambda_{t_{x-1}}\right)}{t_x-t_{x-1}}$. For simplicity, we consider the specific case in which all time intervals are of the same length.
\\


\subsubsection*{Comparisons between two sets of radiocarbon data}

Some research questions requiere the comparison of two sets of samples of radiocarbon data (\emph{e.g.}\ comparison of two geographical regions, or different food sources on the same region). For two sets $\mathrm{a}$ and $\mathrm{b}$, the total data is $\bm{R} = \bm{R}^{\mathrm{a}} + \bm{R}^{\mathrm{b}}$.
%In the case that we analyse, we consider dated samples of wheat ($\mathrm{w}$) and barley ($\mathrm{b}$) and we refer to their total as the dated archaeological record of ``cereals'' (but note that there are other cereal samples in the data that are excluded from the analysis for the sake of simplicity).
Each set of radiocarbon data can be modelled with a Poisson distribution: $R^{\mathrm{a}}_t \sim \mathrm{Pois}(\lambda^{\mathrm{a}}_t)$, $R^{\mathrm{b}}_t \sim \mathrm{Pois}(\lambda^{\mathrm{b}}_t)$, and $R_t \sim \mathrm{Pois}(\lambda_t) = \mathrm{Pois}(\lambda^{\mathrm{a}}_t + \lambda^{\mathrm{b}}_t)$. We define $q_t=\frac{\lambda^{\mathrm{a}}_t}{\lambda_t}$, which describes the proportion of category $\mathrm{a}$ contributing to the total amount of samples. Our interest here is to understand whether the relationship of the changes of $\lambda^{\mathrm{a}}_t$ and $\lambda^{\mathrm{b}}_t$ with time is determined by some common factors or if their histories are independent. We consider three scenarios for the relationship between two sets of samples. In the first scenario, that we name ``independent'', $\lambda^{\mathrm{a}}_t$ and $\lambda^{\mathrm{b}}_t$ values are independent. In the second scenario, that we name ``interdependent'', parameters $\lambda_t$ and $q_t$ determine $\lambda^{\mathrm{a}}_t=q_t\lambda_t$ and $\lambda^{\mathrm{b}}_t=(1-q_t)\lambda_t$. The third scenario, that we name ``parallel'', is a special case of the interdependent scenario in which $q_t$ is constant through time. The dependency among parameters in these scenarios is obtained through conditional prior probability distributions among $\bm{\lambda}^{\mathrm{a}}$, $\bm{\lambda}^{\mathrm{b}}$ and $\bm{q}$ (see below).
\\


%To address this question we can model the changes in $\lambda_t$ through time with models such as those described in the preceding paragraph; and define a new set of parameters for the proportion of the different contributing categories: . For categories $\mathrm{w}$ and $\mathrm{b}$ following exactly the same history, $q^{\mathrm{w}}_t$ is a constant, while variations of $q^{\mathrm{w}}_t$ through time represents divergent histories for each category.
%This approach can also be extended to model more than two categories.
%\\



\subsubsection*{Model of probabilities}

Previous works have considered similar models (\emph{e.g.}\ exponential change) to describe a probability distribution from which a fixed number of radiocarbon dates are sampled \parencite{Porcic2020, Crema2021b, Timpson2020}. These `probability distribution' models, describe the change of probability $\pi$ through time instead of the change of $\lambda$ through time. For any of the models of counts, an equivalent model of probabilities can be obtained by setting $\pi_t=\dfrac{\lambda_t}{ \displaystyle\sum\limits_{t_{\min}}^{t_{\max}} \lambda_t }$, so the total probability of the model equals one for the period considered. It is important to note that by doing this normalization the model of probabilities has one degree of freedom less than the  model of counts. For instance, the exponential count model has parameters $\lambda_0$ and $r$, while the exponential probability model is determined solely by $r$ (there is a single possible value of $\pi_0$ for each value of $r$). Also, the probability model is restricted to the studied period (formally, the probability outside the rage is zero), while the count model can be extrapolated beyond that period of time.
\\




%Other models governing the change of $\lambda$ with time can be considered, such dynamic models with an exponential growth rate that changes through time depending on the population size and an environmental covariate \parencite[\emph{e.g.}][]{Brown2017,DiNapoli2021}. These models will not be explored in this work for the sake of simplicity.

%

\subsection*{Inference using approximate Bayesian computation}

Approximate Bayesian Computation (ABC) is a statistical approach to make model-based inference without the calculation of likelihoods \parencite[see][for a review]{Sunnaker2013}. ABC is often used for inference under models with analytically intractable likelihoods, which is not necessarily the case for the models of abundance of radiocarbon samples \parencite[\emph{e.g.}][]{Crema2021b}. However, it has other advantages such as the fast implementation under different models and priors, which is one of the main reasons for its use in this work (see below for a discussion of other reasons). In ABC, the calculation of the likelihood of a model is substituted by the simulation of data under the model. The similarity between the real and simulated data reflects the likelihood of the model.
\\

\subsubsection*{Summary statistics}

The similarity between the real and simulated data  is typically evaluated by comparing several summary statistics of the data. Previous applications of ABC to the analysis of the abundance of radiocarbon samples used the values of the SPD at each year as summary statistics \parencite{Porcic2020,DiNapoli2021}. In this work we also explore the alternative of using summary statistics based directly on CRAs (\emph{i.e.}\ $\bm{R}'$). Specifically we use: $T$, the total number of uncalibrated dates; $H_{u_i}$, the number of uncalibrated dates at interval $\left[u_i, u_i-\delta\right)$, with $u_{i+1}=u_i+\delta$ (with values covering the whole period of analysis) and using several values of $\delta$ ($10, 50, 100, 500$); and $\Delta H_{u_i}$, the difference between consecutive $H_{u_i}$ and $H_{u_{i+1}}$ values. A visual example of $H_{u_i}$ statistics is the histogram of CRA from Britain and Ireland in figure~\ref{fig:data}.
\\


\begin{figure}
\center\includegraphics[width=12cm]{../../results/Bevan_all/hist.pdf}
%\center\includegraphics[width=12cm]{../../results/spd.pdf}
\caption[Figure shows an histogram of the number of radiocarbon samples in bins of 100 years between 10000 and 5000 uncalibrated YBP. The height of the bars are low (below 50) for more ancient times until around 5000 uncalibrated YBP when a strong increase is apparent with bars reaching values around 300, followed by a small decrease. A second increase occurs aroung 4000 uncalibrated YBP, with bars reaching values around 500.]{\textbf{Conventional radiocarbon ages composing the dated archaeological record from Britain and Ireland \parencite{Bevan2017a}.} Histogram with number of radiocarbon samples in bins of 100 (uncalibrated) years \parencite[samples from the same site are weighted similarly to the procedure proposed by][]{Shennan2013}. This is a visual representation of summary statistics $H_{u_i}$ with $u_i$ taking values from $9900$ to $500$ YBP and $\delta=100$.}
\label{fig:data}
\end{figure}


In the case of real archaeological data, dates belonging to the same site are given a lower weight for the calculation of all these statistics. This is done to compensate biases caused due to large variance in sample size among sites that could reflect, for instance, differences in the resources or research questions of the teams working on them rather than the abundance of materials. These weights are calculated by using the binning procedure proposed by \textcite{Shennan2013}. The weight for each uncalibrated date is the inverse of the number of dates within the bin. For instance, all the dates within a bin count as a single sample in $T$. Here we have used a binning range of 100 years.
\\


In the case of the analysis of two sets of radiocarbon data, we define additional summary statistics. These additional statistics capture the relationship between the two sets in their the abundance of samples or its change. This is captured by the calculation of the correlation and covariance between $\bm{H}^\mathrm{a}$ and $\bm{H}^\mathrm{b}$, and between $\bm{\Delta H}^\mathrm{a}$ and $\bm{\Delta H}^\mathrm{b}$ (for sets $\mathrm{a}$ and $\mathrm{b}$).
\\




\subsubsection*{Approximate Bayesian computation \emph{via} random forests}

Previous applications of ABC to the analysis of abundance of radiocarbon samples have used the ABC rejection algorithm \parencite{Porcic2020,DiNapoli2021}. This algorithm represents the most basic way of performing ABC and presents several limitations respect to other algorithms proposed for the comparison of observed and simulated summary statistics. Here, we use ABC \emph{via} Random Forests \parencite[ABCRF;][]{Pudlo2016, Raynal2019}, which uses the eponymous machine learning algorithm to learn the relationship between summary statistics similarity and posterior probability of the model or the parameters. In the learning step, random forests are grown from a training set constituted by a large number of simulations known as the reference table. One random forest is grown for each parameter or for each model comparison and they can be used to make predictions about the real data. An important advantage of this algorithm is that a lower number of simulations are required for inference (reducing the computational cost) and there is no need to set an arbitrary tolerance level. 
\\

%Because of the way random forests are grown, they can also be used to evaluate the performance of the method. Random forests are a collection of decision trees that are grown from random subsets of the training data (the reference table in the case of ABC), in a processed called bootstrap aggregating or ``bagging''. Because of this, for each simulation in the reference table there is always a subsets of trees in the random forest that have been grown without the information of that simulation. That subset of trees can be used to make inferences for that simulation, which are called the ``out-of-bag'' predictions (OOB). The true values and the out-of-bag predictions can be compared to estimate the error of the method, without the need of an additional testing set, providing confusion matrices for model choice evaluation or mean squared error for parameter inference evaluation.
%\\


% For each simulation, the reference table contains the parameter values and the summary statistics obtained after the simulation. In order to create the reference table it is necessary to set (i) a procedure to choose the parameter values for the simulation, (ii) an algorithm to simulate the data and (iii) summary statistics to describe the data. The application of the ABC approach allows to perform model comparison, in order to select the model with highest posterior probability, and parameter inference (\emph{i.e.}\ obtain parameter estimates and credibility intervals from their posterior probability distribution).

\subsubsection*{Simulation}

The simulation of radiocarbon data requires to set a specific model for the relationship between $\lambda_t$ with time (\emph{e.g.}\ the logistic model) and the values of its parameters (\emph{e.g.}\ $\lambda_0^*$, $r^*$ and $K^*$, for the logistic model; where $^*$ denotes simulation values). These will determine all values in $\bm{\lambda}^*$, which are then used to simulate $\bm{R}^*$ by sampling from Poisson distributions. The uncalibrated date $u^*$ for each sample of know date $t^*$ in $\bm{R}^*$ will be simulated by sampling from a Normal distribution with mean and standard deviation taken from the CRA and error associated to $t^*$ in the appropriate calibration curve \parencite{Shennan2013}. This will result in $\bm{R}'^*$. An example of this procedure is presented in figure~\ref{fig:model}. This simulation process is rather similar to the procedure proposed by \textcite{Shennan2013} and widely used in other works. The main difference is that the total number of samples in the simulated data set depends on the model, allowing to account for this additional source of stochasticity. The IntCal20 calibration curve \parencite{Reimer2020} is used throughout this paper.
\\


\subsection*{Evaluation of the methods}

The performance of the method can be evaluated on simulated data for which the generating model and  parameter values are known. This is done by exploiting the properties of the random forest algorithm. Random forests are a collection of decision trees that are grown from random subsets of the training data (the reference table in the case of ABC), in a processed called bootstrap aggregating or ``bagging''. Because of this, for each simulation in the reference table there is a subsets of trees in the random forest that have been grown without the information of that simulation. That subset of trees can be used to make inferences for that simulation, which are called the ``out-of-bag'' (OOB) predictions. True values and OOB predictions can be compared to estimate the error of the method, without the need of an additional testing set. In the context of model choice, OOB error is used to provide confusion matrices. In the context of parameter inference, OOB predictions are used to calculate mean squared error and the correlation coeficient between true values and their corresponding OOB prediction.
\\

\subsubsection*{Choice of summary statistics}

The reduction of the data to a set of summary statistics can entrain the loss of information for the ABC. Therefore, it is recommended to use a set of summary statistics that are informative about the model and parameters to be inferred. Using the SPD \parencite[as in][]{Porcic2020,DiNapoli2021} is a logical choice since SPD is considered to be highly informative about the changes in abundance of radiocarbon samples. However, the calculation of SPD is computationally costly. Also, strictly speaking, the SPD is not a summary of the data but a combination of the data with the calibration curve. Here I propose an alternative set of summary statistics based on the CRA data as described above.
\\

It is important to determine if these summary statistics are as informative as the SPD for the inference and if there is a gain in computational time by using them. First, the computational time for the calculation of the two sets of summary statistics was measured in 300 simulated datasets of 1343 CRA dates. This simulated CRA datasets were generated by sampling 1343 calibrated dates uniformly between $7000$ and $5000$ YBP and using a CRA error of 30 years for simulating their corresponding CRA. The benchmarking procedure compares the SPD calculation as implemented in \texttt{R} package \texttt{rcarbon} \parencite{Crema2021a}, and an implementation of the new set of statistics in \texttt{R} \parencite{Navascues2023}. Then, the performance of the two sets of summary statistics for ABC inference was also evaluated. A reference table of 20000 simulations was produced using the probability distribution model with  probability changing exponentially between $7000$ and $5000$ YBP \parencite[\emph{i.e.}\ the same model used in the ABC example in][]{Crema2022}. The growth rate parameter, $r$, was sampled from a uniform prior distribution between $-0.01$ and $0.01$. Two random forest models with 5000 trees were trained from this reference table, one using the SPD as predictors and another one using the new set of summary statistics ($T$, $H_{u_i}$ and $\Delta H_{u_i}$).
\\

\subsubsection*{Model of probabilities \emph{versus} model of counts}

The effect of using the a model of counts  instead of using a model of probabilities is studied by generating one reference table from each of the two model under exponential change on the period from $7000$ YBP to $5000$ YBP. Parameter values are sampled from the following prior probability distributions: uniform between $-0.005$  and $0.005$ for $r$, and log-uniform between $0.005$ and $5$ for $\lambda_0$. A condition of $\displaystyle\sum\limits_{t=0}^{2000}\lambda_0e^{-rt}<5000$ is imposed to avoid simulations with an unrealistic high value of samples. For each parameter value combination, $r^*$ and $\lambda_{0}^*$, two simulations are run, one for each of the two separate reference tables. The first simulation uses $r^*$ and $\lambda_{0}^*$ to simulate under the model of counts and the second uses only $r^*$ to simulate 1343 CRA dates from a model of probabilities. Summary statistics based on the CRA data ($T$, $H_{u_i}$ and $\Delta H_{u_i}$) are calculated and random forests are grown for $r$ and $\lambda_0$ for the count model, and $r$ and $\pi	_0$ the probability model (note that the estimation of $\pi_0$ is done for comparison with $\lambda_0$ but is unnecesary in practice if parameter $r$ has already been estimated).% Note that inference of $p_0$ is redundant with the inference of $r$, this is done for easier comparison and interpretation of the results among the two models.
\\

In addition to the evaluation through the OOB predictions, a separate set of independent simulations (pseudo-observed data-sets, PODs) was produced to study the properties of the posterior distributions obtained under the two different models. These PODs were simulated in a model of counts with $\lambda_0=0.01$, exponential rate change of $r=0.003$, with starting time $7000$ YBP and final time $5000$ YBP. Simulations were run until 300 PODs were obtained that contained exactly 1343 samples (the expected number of samples under that model%: $\displaystyle\sum\limits_{t=0}^{2000}\lambda_0e^{-rt}$
). Conditioning the simulation to a specific number of samples was done in order to analyse those PODs with the reference tables from both models (counts and probabilities) since the probabilities model assumes a fixed number of samples. The first 300 PODs were used to estimate posterior probability distributions for $r$ under the model of counts and the PODs with 1343 samples were used to estimate posterior probability distributions for $r$ under the model of probabilities.
\\


\subsection*{Case study: archaeological radiocarbon dates from Britain and Ireland}

In order to illustrate the approach presented in this work we reanalyse data of archaeological radiocarbon dates from Britain and Ireland \parencite{Bevan2017b}. This data base comprises 30516 radiocarbon dates from 200 to 9580 uncalibrated YBP from Ireland (7797 entries), Scotland (6401 entries), North-West England and Wales (5333 entries), and South-East England (10985 entries). In more than three quarters of the entries, the taxonomic origin of the material is identified. The taxonomic level of this identification is heterogeneous across the data: sometimes identification is at species level but often it is only at genus or higher levels. Among the taxon identified, there are several food sources, such as as wheat (\emph{Triticum}, 678 entries) and barley (\emph{Hordeum}, 1102 entries). 
\\

The original article by \textcite{Bevan2017a} studies the change of human population size and usage of food ressources based on those data. Our work is not intended as a thorough reanalysis of this dataset but as an illustration of the model and method proposed. Therefore, we only focus on two questions: the global pattern of change in abundance of radiocarbon samples (interpreted as a population size proxy in the original article) and the relationship between the abundance of samples of barley and wheat through time.
\\

\subsubsection*{Estimation of population size changes in Britain and Ireland}

For the analysis of the population size change in Britain and Ireland, we consider the three models described above: expontential change, logistic change and piecewise exponential change. The time period explored is restricted between $10000$ and $500$ YBP. For the exponential and the logistic models, the parameters $\lambda_0$ and $\lambda_\mathrm{f}$ (value of $\lambda$ at $500$ YBP) were taken from a log-uniform prior distribution in the range $\left[ 0.001,12\right]$, conditional to histories of increasing $\lambda$ ($\lambda_0 < \lambda_\mathrm{f}$). For the logistic model, parameter $k$ value was sampled from a log-uniform distribution in the range $\left[\lambda_\mathrm{f}+0.001,\lambda_\mathrm{f}+12\right]$. Rate of change $r$ is obtained from the values of those parameters.
\\

In the piecewise exponential model there are $m+1$ parameters $\lambda$ ($\lambda_{t_0},\lambda_{t_1},\dots ,\lambda_{t_{m}}$). The value of $m$ is set to divide the analysed total range of ages in periods of approximately 400 years. Thus, for the range $10000$ to $500$ YBP, $m=24$. The value of $\lambda_{t_0}$ is taken from a log-uniform prior distribution in the range $\left[\lambda_{\min},\lambda_{\max}\right]$ and consecutive values $\lambda_{t_x}=\max(\min(\phi\lambda_{t_{x-1}},\lambda_{\max}),\lambda_{\min})$ with $\phi$ taken from a log-uniform distribution in the range $\left[0.1,10\right]$ \parencite[as in][]{Boitard2016}. This way of sampling the evolution of $\lambda$ through time reflects the prior belief that large jumps over a short period of time are unrealistic (this prior prevents changes larger that one order of magnitude for consecutive $\lambda_{t_x}$ values). The minimum and maximum $\lambda$ values for the whole model are $\lambda_{\min}=0.001$ and $\lambda_{\max}=12$.
\\

For each model, a reference table of 30~000 simulation was build taking parameter values from the prior distibutions described above. Model choice and posterior probability for the observed data were obtained through ABCRF, using 2000 trees for the training of the random forest and 2000 trees for the calculation of the posterior probability. The pertinence of the approach was evaluated in two ways. First, OOB prediction were used to calculate the confusion matrix to evaluate the general perfomance of the approach. Second, a visual evaluation of the goodness-of-fit of the model to the observed data is also provided: the variability of patterns produced by the different models is represented using Principal Component Analysis (PCA) of the summary statistics in the reference table, then the observed data set is projected into the PC space. 
\\

A larger reference table of 100~000 simulation was used to estimate the parameters of each model. Random forest of 2000 trees were trained on $\log(\lambda_0)$, $\log(\lambda_f)$, $\log(k)$ for the exponential and logistic model. For the piecewise exponential model random forest of 2000 trees were trained for $\log(\lambda_{t_x})$ at each the 25 time points defining the periods and $r_x$ for each of the 24 periods.
\\

\subsubsection*{Testing the relationship between abundances of wheat and barley in Britain and Ireland}

For the study of the abundances of wheat and barley, we considered a piecewise exponential model and explored the time range from $6000$ to $500$ YBP diveded in $m=14$ periods of approximately 400 years. The model describes the abundance of radiocarbon samples of two categories: wheat ($\mathrm{w}$) and barley ($\mathrm{b}$). The samples were ascribed to these two categories following the same criteria as in \textcite{Bevan2017a}. The relationship between the changes of abundance through time of these two categories was modelled according to the above mentionned independent, interdependent and parallel scenarios. All three scenarios are produce with the same model, which have 15 parameters $\lambda^\mathrm{w}_{t_x}$ and 15 parameters $\lambda^\mathrm{b}_{t_x}$; the differences among scenarios reside in the conditional prior probability distributions.
\\


For the independent scenario, parameters $\lambda^\mathrm{w}_{t_x}$ and $\lambda^\mathrm{b}_{t_x}$ are sampled independetly using the same procedure as described above. That is, $\lambda^\mathrm{w}_{t_0}$ is sampled from a log-uniform distribution in the range $\left[\lambda_{\min},\lambda_{\max}\right]$ and consecutive values $\lambda_{t_x}=\max(\min(\phi\lambda_{t_{x-1}},\lambda_{\max}),\lambda_{\min})$ with $\phi$ taken from a log-uniform distribution in the range $\left[0.1,10\right]$. For the interdependent scenario, parameters $\lambda_{t_x}$ (\emph{i.e.}\ $\lambda$ for the sum of both categories), are sampled with the same procedure; then $q_{t_x}$ are sampled from a uniform distribution in the range $[0,1]$ which determines the proportion of categories $\lambda^\mathrm{w}_{t_x}$ and $\lambda^\mathrm{b}_{t_x}$ at each time $t_0, t_1, \dots , t_{m}$. Finally, the parallel scenario is a special case of the interdependent scenario, in which a single $q$ value is taken from a uniform distribution in the range $[0,1]$ and the proportion of the two categories does not change through time. The minimum and maximum $\lambda$ values for the whole model are $\lambda_{\min}=0.001$ and $\lambda_{\max}=2$.
\\


%Our work is not intended as a thorough reanalysis of this dataset but as an illustration of the potential of the proposed model and method. I therefore focus on a few of the questions discussed by \textcite{Bevan2017a} trying to provide a more quantitative assessment of the significance and uncertainty of the results. Specifically, I showcase how to apply our approach to (i) identify the model that better describes the abundance of the dated archaeological record, (ii) test the influence of an environmental variable on the dated archaeological record, (iii) identify the periods with a significant rate of change, and (iv) determine the interdependency of changes of abundance among some food sources such as cereals.




%The second element evaluated is the goodness-of-fit of the model to the observed data. The model choice algorithms can identify the model that best explains the data among the models considered. However, it might be the case that all the considered models are bad at explaining the data; that is, they are unable to generate the similar patterns to the ones observed in the data. In that case, the selected model is unlikely to give any relevant information. In order to verify that this is not the case in the analysis, I visualise the variability of patters produce by the different models using Principal Component Analysis (PCA) to the summary statistics in the reference table and project the observed data set into the PC space. The overlapping of the observed data with the simulated data confirms that the proposed models are able to reproduce the observed patterns.




\subsection*{Implementation}


All the calculations presented in this work were done in R \parencite{R2021} with scripts \parencite[available in][]{Navascues2023} that use: package {extraDistr} \parencite{Wolodzko2020} to sample from prior distributions; package {rcarbon} \parencite{Crema2021a} to simulate CRA; packages {Hmisc} \parencite{Harrell2022}, {moments} \parencite{Komsta2022} and {weights} \parencite{Pasek2021}
%and {zoo} \parencite{Zeileis2005}
to calculate summary statistics; and package {abcrf} \parencite{Marin2022} to perform ABC analyses. Simulations are run in parallel using  {doParallel} \parencite{Microsoft2022a}, {doSNOW} \parencite{Microsoft2022b} and {doRNG} \parencite{Gaujoux2023}.
\\


\section*{\centering Results}

\subsection*{The choice of model and summary statistics for ABC inference}

We evaluated the performance of two distinct sets of summary statistics. One set comprises the values of the SPD for each year, while the second set is calculated from counts of uncalibrated dates as detailed in the Methods section. Summary statistics based on counts of uncalibrated dates offer a significant computational advantage, being approximately 250 times faster to calculate. Despite this difference, both sets of summary statistics demonstrate very high accuracy in inference with no discernible difference in statistical results  (Figure~\ref{fig:summary_stats}).
\\



\begin{figure}[tbh]
\center\includegraphics[width=7cm]{../../results/method_evaluation/OOB_p_spd.pdf} \includegraphics[width=7cm]{../../results/method_evaluation/OOB_p_hist2.pdf}
\caption[]{\textbf{Influence of choice of summary statistics on the estimation of parameters.} Out-of-bag (OOB) estimates of the exponential growth rate, $r$, compared to the true value from simulations in the reference table. The ABC was performed either using: (a) the SPD as summary statistics, or (b) a set of summary statistics calculated from the count of CRA. The performance is very similar despite the much higher computational cost of using the SPD.}
\label{fig:summary_stats}
\end{figure}




\begin{figure}[tbh]
\center\includegraphics[width=7cm]{../../results/method_evaluation/OOB_lambda_rate.pdf} \includegraphics[width=7cm]{../../results/method_evaluation/OOB_lambda_0.pdf} \includegraphics[width=7cm]{../../results/method_evaluation/OOB_p_rate.pdf}
\includegraphics[width=7cm]{../../results/method_evaluation/OOB_p_0.pdf}
\caption{\textbf{Parameter inference differences between modeling counts  and modeling probabilities.} Out-of-bag (OOB) estimates compared to the true value from simulations in the reference table. (a) and (b) model of counts. (c) and (d) model of probabilities. (a) and (c)  growth rate, $r$. (b) $\lambda_0$, (c) $\pi_0$.}
\label{fig:freq_vs_counts}
\end{figure}


\begin{figure}[tbh]
\center\includegraphics[width=7cm]{../../results/method_evaluation/plot_CI_coverage.pdf}\includegraphics[width=7cm]{../../results/method_evaluation/plot_95CI_width.pdf}
\caption{\textbf{Influence of the model on the credibility interval width.} Relative width of the 95\% credibility interval of the parameter $r$ estimated under a model of probabilities or under a model of counts. The histogram represent 300 values from pseudo-observed data-sets generated with a model of exponential change with $r=0.003$ and $\lambda=0.01$.}
\label{fig:95CIwidth}
\end{figure}


We also evaluated the use of two different models, referred to as the model of probabilities and the model of counts, each offering a distinct perspective on the process generating radiocarbon data. Notably, parameter inference under the model of probabilities demonstrated higher accuracy compared to the model of counts (see Figure~\ref{fig:freq_vs_counts}). This discrepancy in accuracy primarily stems from larger errors observed in simulations with low values of $r$ or $\lambda_0$ (refer to Figure~\ref{fig:freq_vs_counts}a and b), which consequently results in less data for the model of counts.
\\

For PODs generated under the model of counts ($\lambda_0=0.01$, $r=0.003$, with a range of $2000$ years), analysis under either the model of probabilities or the model of counts yielded comparable levels of error (mean squared error of $9.57\times10^{-9}$ and $9.31\times10^{-9}$ respectively for parameter $r$). However, it's worth noting that the 95\% credibility intervals were wider for the model of counts (refer to Figure~\ref{fig:95CIwidth}b). Furthermore, nominal coverage was more accurate for the model of probabilities (see Figure~\ref{fig:95CIwidth}a).
\\




\subsection*{Analysis of archaeological radiocarbon dates from Britain and Ireland}

\subsubsection*{Estimation of $\lambda$ as a proxy of population size}

Three models (exponential, logistic and piecewise exponential) were explored to explain the change in abundance of radiocarbon samples from Britain and Ireland. According to the OOB estimates from the training set, the ABCRF approach is able to identify the piecewise exponential model with very little error and the logistic model with a somehow higher error. However, the exponential model is difficult to identify, being often wrongly classified as the logistic model (table~\ref{tab:confusion_matrix}). For the real data, the piecewise exponential model has a clear superior fit than the two alternative models, with the ABRF analysis indicating a high posterior probability for that model ($0.869$). The PCA of the summary statistics diversity across simulations further reveals the lack of fit of the exponential and logistic models, which are unable to reproduce the patterns found in real data (figure~\ref{fig:PCA_all}b and c). Parameter estimates under the piecewise model reveal a history of fluactuations of $\lambda$ through time closely ressembling the SPD curve (figure~\ref{fig:piecewise_results}a). Five of the periods ($6833$--$6438$, $6042$--$5646$, $4854$--$4458$, $4458$--$4062$ and $1688$--$1292$ YBP) have estimates of the rate of change $r$ with credibility intervals excluding the zero, indicanting a significant increase of $\lambda$ during those periods (figure~\ref{fig:piecewise_results}b). The posterior probability distribution for all the parameters estimated is clearly distict from the prior probability distribution used to train the ABCRF algorithm (figures~\ref{fig:posterior_piecewise_lambda} and \ref{fig:posterior_piecewise_rate}).\\


\begin{figure}[tbh]
\center\includegraphics[width=12cm]{../../results/Bevan_all/piecewise_model_result_lambda.pdf}
\center\includegraphics[width=12cm]{../../results/Bevan_all/piecewise_model_result_rate.pdf}
\caption{\textbf{Parameter estimates under piecewise exponential model.} (a) Abundance of dated archaeological record through time measured as the expected number of dated archaeological samples per year ($\lambda$). Solid blue line indicate the point estimate ($\hat\lambda$) and dashed lines indicate 95\% credibility interval. The Sum of Probability Densities (grey line) of the data is plotted for reference. (b) Rate of change in the abundance of dated archaeological record through time ($r$). Solid blue line indicate the point estimate ($\hat{r}$) and dotted lines indicate 95\%CI. Periods in which the 95\%CI for $r$ does not include zero (horizontal grey line) are marked with an asterisk (\textbf{*}).}
\label{fig:piecewise_results}
\end{figure}


\subsubsection*{Relationship between the abundances of radiocarbon samples from wheat and barley}

The change of abundance of radiocarbon samples for wheat and barley was modeled with a piecewise exponential model. Three different scenarios within than model were considered according to the degree of independence between the trajectories of the two cereals: the independent scenario, where both trajectories are completely independent; the parallel scenario, where both trajectries are parallel in their changes; and the interdependent, where both trajectories are correlated. The three scenarios can be distinguished with relatively low error through ABCRF according to the confusion matrix (table~\ref{tab:confusion_matrix}). For the emprircal data the chosen scenario is the interdependent scenario with a posterior probability of $0.863$.



\begin{figure}[tbh]
\center\includegraphics[width=12cm]{../../results/Bevan_cereals/interdependent_model_result_lambda.pdf}
\center\includegraphics[width=12cm]{../../results/Bevan_cereals/interdependent_model_result_q.pdf}
\caption{\textbf{Parameter estimates under the interdependent scenario for cereals (\textit{Hordeum}/\emph{Triticum}).} (a) Estimate of the expected number of \textit{Hordeum} and \emph{Triticum} samples through time ($\lambda$). Solid blue line indicate the point estimate ($\hat\lambda$) and dashed lines indicate 95\% credibility interval. The Sum of Probability Densities (grey line) of for \emph{Triticum} (solid line) and \textit{Hordeum} (dashed line) are plotted for reference. (b) Proportion of \emph{Triticum} samples ($q$) contributing to the total number of samples ($\lambda$). Solid blue line indicate the point estimate ($\hat{q}$) and dotted lines indicate 95\%CI.}
\label{fig:cereals_interdependent_results}
\end{figure}




\section*{\centering Discussion}




ABC is widely used in the analysis of population genetic data to make inferences on demography. Assuming that the dated archaeological record is informative about the same demographic, both types of data could be analysed together using ABC. For instance, genetic data from \parencite[][]{Patterson2022} and the radiocarbon data used here \parencite{Bevan2017a} could be analysed together to study the demography during the Neolithic transition in Britain. 
\\


Many archaeological samples are not (or cannot be) radiocarbon dated. However, other approaches are sometimes used (numismatic dating, context) to estimate their age. If an appropriate mathematical description of the uncertainty of the dating is provided, it might be possible to analyse such data together with radiocarbon dated samples.
\\


Demographic inference from radiocarbon data assumes that there is a relationship between the magnitude of the population size ($N_t$) and the magnitude of dated archaeological samples at a given time $t$. Mathematically we can express this as $\lambda_t \propto N_t$. This parameter $\lambda$ describes a complex reality: the reduction of the total amount of carbon-based items in the human settlement (wood, tools, food, humans themselves, etc.)\ to the amount that is deposited, then preserved though time, then excavated/discovered and finally dated. It has been acknowledged that each of these steps can be the source of biases: the amount of fire used (thus charcoal deposit) changes with climate, younger samples are more likely to be preserved than older ones, archaeological research questions drive which sites/periods are studied and which samples are dated \parencite{Rick1987,Williams2012}. However, it is assumed that these biases do not distort the general patterns and some attenuation procedures have also been proposed \textcite[\emph{e.g.}\ binning][]{Shennan2013}.
\\



% TAPHONOMIC BIAS
% If we consider the possibility of taphonomic bias, only a proportion of those items will perdure into the present. The probability $p$ that any item remains in the archaeological deposit at present depends on time $p_t = e^{-\lambda t}$ (older items are more likely to disappear). The final number of items contributed to the archaeological record from year $t$ can be modeled with a Binomial distribution $n'_t \sim B(n_t,p_t)$





\section*{\centering Acknowledgements}


This work has greatly benefited from insightful discussions with several individuals, and we would like to express our gratitude to Jan Apel, Kristian Brink, Magdalena Fraser, Anders H√∂gberg, Helena Malmstr√∂m, and Rita Peyroteo Stjerna for their valuable feedback. Additionally, we acknowledge the collaborative efforts facilitated by the memorandum of understanding between Uppsala University (IOB and EUG departments) and the INRAE (UMR CBGP). This agreement has played a pivotal role in strengthening the collaboration between these institutions and has significantly contributed to the advancement of this research.
\\

\section*{\centering Funding}

This project has received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 791695 (TimeAdapt).
\\


\section*{\centering Conflict of interest disclosure}

The authors of this article declare that they have no financial conflict of interest with the content of this article.
\\
% Miguel de Navascu√©s is recommender for PCI Evolutionary Biology.

% \section*{\centering Data, script and code availability}

% Data are available online: DOI of the webpage hosting the data (eg \url{https://doi.org/10.24072/pcjournal.125} or other link)

\section*{\centering Supplementary information availability}

Scripts to reproduce the analyses presented here are available at Zenodo \parencite{Navascues2023}. Data used in this work are from previous publications and were made available by \textcite{Bevan2017b}.
\\


\section*{\centering \href{https://credit.niso.org/}{\textcolor{black}{Author contributions}}}

MdN (Conceptualization, Formal analysis, Funding acquisition, Investigation, Methodology, Project administration, Software, Validation, Writing --- original draft), CB (Conceptualization, Writing --- review \& editing), MJ (Conceptualization, Funding acquisition, Resources, Writing --- review \& editing).
\\


% Conceptualization % Ideas; formulation or evolution of overarching research goals and aims.

% Data curation % Management activities to annotate (produce metadata), scrub data and maintain research data (including software code, where it is necessary for interpreting the data itself) for initial use and later re-use.

% Formal analysis % Application of statistical, mathematical, computational, or other formal techniques to analyse or synthesize study data.

% Funding acquisition % Acquisition of the financial support for the project leading to this publication.

% Investigation % Conducting a research and investigation process, specifically performing the experiments, or data/evidence collection.

% Methodology % Development or design of methodology; creation of models.

% Project administration % Management and coordination responsibility for the research activity planning and execution.

% Resources % Provision of study materials, reagents, materials, patients, laboratory samples, animals, instrumentation, computing resources, or other analysis tools.

% Software % Programming, software development; designing computer programs; implementation of the computer code and supporting algorithms; testing of existing code components.

% Supervision % Oversight and leadership responsibility for the research activity planning and execution, including mentorship external to the core team.

% Validation % Verification, whether as a part of the activity or separate, of the overall replication/reproducibility of results/experiments and other research outputs.

% Visualization % Preparation, creation and/or presentation of the published work, specifically visualization/data presentation.

% Writing --- original draft % Preparation, creation and/or presentation of the published work, specifically writing the initial draft (including substantive translation).

% Writing --- review \& editing % Preparation, creation and/or presentation of the published work by those from the original research group, specifically critical review, commentary or revision ‚Äì including pre- or post-publication stages.

\titleformat*{\section}{\bfseries\Large\centering}

%%%%%% if they exist, DOIs are required %%%%%%%
\printbibliography[notcategory=ignore]



\newpage
\section*{\centering Supplementary Materials}

\renewcommand\thesubsection{S.\arabic{subsection}}
\setcounter{table}{0}
\setcounter{figure}{0}
\renewcommand\thefigure{S\arabic{figure}}
\renewcommand\thetable{S\arabic{table}}

\subsection{An interpretation of Sum of Probability Distributions (SPD) as the expected number of samples per year}\label{supplementary_text}

As far as we know, a formal mathematical interpretation of the SPD is lacking, as the algorithm defining it lacks a rigorous mathematical justification for aggregating independent probability distributions. Nevertheless, we find it more intuitive to interpret the SPD as the expected number of samples from each year rather than as a probability distribution. Consider the meaning of the SPD value at a specific year. Each sample in the radiocarbon data set has a probability of being 'sampled' in that year, given by the posterior probability distribution from the calibration process. Consequently, each sample actually originating from that year can be viewed as a 'success¬¥ and samples from other years as 'failures¬¥ akin to independent binomial trials. The sum of these successes, \emph{i.e.}, the number of samples from that year, follows a Poisson-binomial distribution. The expected value of a Poisson-binomial distribution (\emph{i.e.}\ the expected number of samples at that year) is the sum of the probabilities of each trial, representing the SPD value for that year. This rationale provides the insight that the SPD somehow quantifies the expected number of samples for each year, suggesting that the analysis of radiocarbon abundance data should focus on modeling the number of samples at each year. However, SPD values from different years are not independent, rendering the Poisson-binomial model inapplicable to the entire SPD. Remarkably, our proposed model, utilizing Poisson distributions to model the number of samples at each year, yields inferences closely aligning with SPD values (figure~\ref{fig:piecewise_results}).
\\

\begin{table}[h]
\caption{\textbf{Notation\textsuperscript{a}}}
\label{tab:notation}
\small
\begin{tabularx}{\textwidth}{lX}
\hline
& meaning \\
\hline\hline
CRA & conventional radiocarbon age\\
$H_{u}$ & number of samples in an interval starting at uncalibrated year $u$ and ending at uncalibrated year $u+\delta$\\
$k$ & upper bound value of $\lambda$ under a model of logistic change\\
$n_t$ & number of objects in year $t$ that can potentially become a radiocarbon sample in the data set\\
$\mathcal{N}$ & normal or Gaussian distribution\\
$p_t$ & probability of an object to become a sample in the radiocarbon data at year $t$\\
$q$ & ratio between $\lambda$ of subset $\mathrm{a}$ ($\lambda^\mathrm{a}$) and $\lambda$ for the total dataset\\
$r$ & growth rate of $\lambda$ under a model of exponential or logistic change\\
$\bm{R}$ & vector of number of radiocarbon samples for each year between $t_\mathrm{min}$ and $t_\mathrm{max}$\\
$\bm{R}^{\mathrm{a}}$ & vector of number of radiocarbon samples for each year between $t_\mathrm{min}$ and $t_\mathrm{max}$ for the subset of samples $a$\\
$R_t$ & number of radiocarbon samples at year $t$, an element of vector $\bm{R}$\\
$\bm{R'}$ & vector of number of radiocarbon samples with CRAs between $u_\mathrm{min}$ and $u_\mathrm{max}$\\
$R'_u$ & number of radiocarbon samples with CRA$=u$, an element of vector $\bm{R'}$\\
$t$ & time in years (calibrated)\\
$T$ & total number of samples in the radiocarbon data\\
$u$ & uncalibrated radiocarbon year (measurement unit for CRA)\\
$\delta$ & size of the interval of uncalibrated years used to calculate summary statistics of the data\\
$\Delta H_{u}$ & Difference between values $H_{u}$ and $H_{u+\delta}$\\
$\bm{\lambda}$ & vector of expected number of radiocarbon samples for each year between $t_\mathrm{min}$ and $t_\mathrm{max}$\\
$\lambda_t$ & expected number of radiocarbon samples at year $t$, an element of vector $\bm{\lambda}$\\
$\pi_t$ & probability that the age of a sample is $t$\\
\hline
\end{tabularx}\\
\footnotesize{\textsuperscript{a} We follow the convention of marking vectors with bold font.}\\\end{table}



\begin{table}[tbh]
\caption{\textbf{Confusion matrix}}
\label{tab:confusion_matrix}
\small
\begin{tabular}{lrrrr}
\hline
true model &         & prediction &           & error rate \\
\hline\hline
            & exponential        & logistic           & piecewise          &  \\
\hline
exponential & \customcell{15067} & \customcell{14873} & \customcell{60}    & $0.498$ \\
logistic    & \customcell{4631}  & \customcell{25277} & \customcell{92}    & $0.157$ \\
piecewise   & \customcell{132}   & \customcell{324}   & \customcell{29544} & $0.015$ \\
\hline\hline
               & independent        & interdependent     & parallel           &   \\
\hline
independent    & \customcell{26348} & \customcell{2480}  & \customcell{588}   & $0.104$ \\
interdependent & \customcell{980}   & \customcell{25290} & \customcell{3146}  & $0.140$ \\
parallel       & \customcell{619}   & \customcell{2479}  & \customcell{26318} & $0.105$ \\
\hline
\end{tabular}
\end{table}







%\begin{figure}[h]
%\begin{center}
%\includegraphics[width=12cm]{../../results/Bevan_all/SPD_plus_lambda_models.pdf}
%\end{center}
%\caption{\textbf{Relationship between $\lambda$ and time under the three models considered.} Based on the parameter point estimates for each model. The Sum of Probability Densities (SPD) of the data is shown for reference.}
%\label{fig:skylines}
%\end{figure}


\begin{figure}[h]
\begin{center}
\includegraphics[width=5cm]{../../results/Bevan_all/PCA_PC1_PC2.pdf}\includegraphics[width=5cm]{../../results/Bevan_all/PCA_PC3_PC4.pdf}\includegraphics[width=5cm]{../../results/Bevan_all/PCA_PC5_PC6.pdf}
\end{center}
\caption{\textbf{PCA for goodness-of-fit evaluation.} PC values from 3000 randomly selected simulations are plotted for each model for the first six axes. The projection of the observed summary statistics is represented by an asterisk (\textbf{*}). The first six principal components capture 97.06\% of the variance in the data and are presented by consecutive pairs in panels (a), (b) and (c).}
\label{fig:PCA_all}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=5cm]{../../results/Bevan_cereals/PCA_PC1_PC2.pdf}\includegraphics[width=5cm]{../../results/Bevan_cereals/PCA_PC3_PC4.pdf}\includegraphics[width=5cm]{../../results/Bevan_cereals/PCA_PC5_PC6.pdf}
\end{center}
\caption{\textbf{PCA for goodness-of-fit evaluation, cereals.} PC values from 3000 randomly selected simulations are plotted for each model. The projection of the observed summary statistics is represented by an asterisk (\textbf{*}). The first six principal components capture 99.60\% of the variance in the data and are presented by consecutive pairs in panels (a), (b) and (c).}
\label{fig:PCA_cereals}
\end{figure}


\begin{figure}[h]
\begin{center}
\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_1_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_2_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_3_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_4_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_5_piecewise.pdf}
\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_6_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_7_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_8_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_9_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_10_piecewise.pdf}
\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_11_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_12_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_13_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_14_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_15_piecewise.pdf}
\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_16_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_17_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_18_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_19_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_20_piecewise.pdf}
\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_21_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_22_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_23_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_24_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_lambda_25_piecewise.pdf}
\end{center}
\caption{\textbf{Posterior distribution for parameters of the piecewise exponential model.} For the sake of simplicity, only a selection of parameters is presented. Sub-indices for $\lambda$ and $r$ parameters indicate the year or year interval (BP, calibrated time).}
\label{fig:posterior_piecewise_lambda}
\end{figure}


\begin{figure}[h]
\begin{center}
\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_1_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_2_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_3_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_4_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_5_piecewise.pdf}
\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_6_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_7_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_8_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_9_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_10_piecewise.pdf}
\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_11_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_12_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_13_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_14_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_15_piecewise.pdf}
\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_16_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_17_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_18_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_19_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_20_piecewise.pdf}
\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_21_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_22_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_23_piecewise.pdf}\includegraphics[width=3cm]{../../results/Bevan_all/posterior_rate_24_piecewise.pdf}
\end{center}
\caption{\textbf{Posterior distribution for parameters of the piecewise exponential model.} For the sake of simplicity, only a selection of parameters is presented. Sub-indices for $\lambda$ and $r$ parameters indicate the year or year interval (BP, calibrated time).}
\label{fig:posterior_piecewise_rate}
\end{figure}



%\begin{figure}[tbh]
%\center\includegraphics[width=7cm]{../../results/piecewise_OOB_lambda.pdf}\includegraphics[width=7cm]{../../results/piecewise_OOB_rate.pdf}
%\includegraphics[width=6cm]{../../results/cereals_OOB_pi.pdf}
%\caption{\textbf{Performance of parameter estimation evaluated using out-of-bag estimates.} For each simulation of the reference table of the piecewise exponential model the out-of-bag estimate is compared with the true simulated value of the parameter. The intensity of the color indicates the number of simulations. For reference, a dashed diagonal line indicates the one-to-one relationship. Mean squared error (MSE) and the coefficient of regression (R2) between true values and estimates are reported within each panel. (a) Performance of the estimation of annual expected number of samples, $\lambda_t$ at time $t=5250$ cal.~years BP (piecewise exponential model). (b) Performance of the estimation of exponential rate of change of $\lambda$, $r$ in the period ($4854$--$4458$) cal.~years BP (piecewise exponential model).%(c) Performance of the estimation of the proportion, $q$, of two classes of samples (wheat and barley) in the cereals dated archaeological record at time  $t=3250$ cal.~years BP (interdependent model). Note that MSE and R2 are calculated on the logit transformation of the parameter $q$.
%}
%\label{fig:OOB}
%\end{figure}



%\begin{figure}
%\includegraphics[width=8cm]{../../results/piecewise_model_result_error.pdf}
%\includegraphics[width=8cm]{../../results/piecewise_model_result_rate_error.pdf}
%\caption{\textbf{Error.} Piecewise model ($m=78$).}
%\label{fig:piecewise_error}
%\end{figure}


\end{document}
